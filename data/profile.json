{
  "profile": {
    "name": "Deepak Rambarki",
    "title": "Senior Data Engineer",
    "organization": "Charles Schwab | 4+ Years Experience | Denton, TX",
    "profileImage": "assets/PIC.jpg",
    "cvPath": "assets/Deepak_DE_Resume.pdf",
    "metrics": [
      "4+ Years Experience",
      "30TB+ Data Migrated",
      "40% Query Latency Reduction",
      "95% Deployment Error Reduction"
    ]
  },
  
  "bio": {
    "introduction": "<span class=\"highlight\">Experienced Data Engineer</span> with <span class=\"highlight\">4+ years of expertise</span> designing scalable data pipelines and analytics solutions across <span class=\"highlight\">finance and tech sectors</span>. Skilled in <span class=\"highlight\">Python, SQL, Spark, Kafka</span>, and cloud platforms (AWS, Azure) to enable <span class=\"highlight\">real-time and batch processing</span>.",
    "background": "Proven ability to optimize data architecture, improve data quality, and support advanced analytics and ML initiatives through robust engineering practices. Expert in <span class=\"highlight\">Apache Spark, PySpark, and Kafka</span> for high-throughput data processing, with extensive experience in <span class=\"highlight\">AWS and Azure cloud platforms</span> for scalable data solutions.",
    "researchFocus": "Specialized in building <span class=\"highlight\">real-time trading data pipelines</span> with sub-2-second latency, <span class=\"highlight\">multi-cloud ETL architectures</span>, and automated compliance reporting systems. Demonstrated success in <span class=\"highlight\">reducing processing times by 60%</span>, <span class=\"highlight\">cutting deployment errors by 95%</span>, and <span class=\"highlight\">improving data accuracy by 35%</span> through advanced engineering practices and cross-functional collaboration."
  },
  
  "contact": {
    "email": "deepakrambarki@gmail.com",
    "phone": "+1 (940) 277-6441",
    "location": "Denton, TX",
    "linkedin": "https://www.linkedin.com/in/rambarki-deepak/",
    "github": null,
    "googleScholar": null,
    "twitter": null,
    "website": null,
    "trailhead": null
  },
  
  "navigation": [
    {"label": "About", "href": "#banner"},
    {"label": "Experience", "href": "#publications"},
    {"label": "Projects", "href": "#projects"},
    {"label": "Skills", "href": "#skills"},
    {"label": "Education", "href": "#education"},
    {"label": "Contact", "href": "mailto:deepakrambarki@gmail.com"}
  ],
  
  "publications": [
    {
      "id": "charles-schwab",
      "title": "Data Engineer",
      "authors": "Charles Schwab | USA",
      "venue": "Dec 2024 - Present",
      "description": "<div class='impact-metrics'><span class='metric'>12+ Real-time Pipelines</span><span class='metric'>&lt;2s Market Data Latency</span><span class='metric'>30TB+ Data Migrated</span><span class='metric'>40% Query Latency Reduction</span></div><br/><strong>Key Achievements:</strong><br/>• <strong>Built 12+ real-time data pipelines</strong> using Apache Kafka and Spark Structured Streaming to support high-frequency trading, <span class='highlight'>reducing market data ingestion latency to under 2 seconds</span> across equities and options processing systems<br/>• <strong>Migrated over 30TB of legacy trading and transaction data</strong> to AWS S3 and Redshift, <span class='highlight'>achieving a 40% reduction in query latency</span> and cutting monthly infrastructure costs<br/>• <strong>Created automated ETL workflows</strong> in Apache Airflow for risk and compliance reporting, <span class='highlight'>improving on-time report delivery across 10 regulatory submission processes</span><br/>• <strong>Engineered dimensional models</strong> for investment performance reporting in Power BI, <span class='highlight'>supporting 100+ financial advisors with near real-time updates</span> and reducing ad-hoc data requests<br/>• <strong>Integrated Spark-based enrichment logic</strong> into batch pipelines to validate trading transactions, <span class='highlight'>increasing data accuracy by 35%</span> while maintaining pipeline execution times under SLA thresholds<br/>• <strong>Enforced access policies</strong> with AWS IAM and audit logging for internal and third-party data consumers, <span class='highlight'>fully aligning with SOX, GDPR, and internal data governance standards</span>",
      "image": "images/charles-schwab-logo.png",
      "links": {
        "paper": null,
        "github": null,
        "status": null
      }
    },
    {
      "id": "adons-softech",
      "title": "Data Engineer", 
      "authors": "Adons Softech | USA",
      "venue": "Jan 2022 - Jul 2023",
      "description": "<div class='impact-metrics'><span class='metric'>40+ Data Sources Integrated</span><span class='metric'>60% Processing Time Reduction</span><span class='metric'>95% Deployment Error Reduction</span><span class='metric'>200M+ Records/Cycle</span></div><br/><strong>Key Achievements:</strong><br/>• <strong>Designed and implemented ETL pipelines</strong> in PySpark and Talend across 5 client environments, <span class='highlight'>integrating 40+ diverse data sources</span> including Salesforce, Oracle DB, and S3 into a centralized Snowflake warehouse<br/>• <strong>Re-engineered existing batch jobs</strong> to leverage Spark parallelism, <span class='highlight'>cutting daily data processing time by 60%</span> while handling workloads exceeding 200 million records per execution cycle<br/>• <strong>Created modular dbt models</strong> with test coverage and version control, <span class='highlight'>improving code reuse across teams</span> and reducing onboarding and documentation time for new engineers<br/>• <strong>Provisioned AWS infrastructure</strong> using Terraform with CI/CD workflows via GitLab, <span class='highlight'>reducing manual deployment errors by 95%</span> and ensuring consistent build environments across dev and production<br/>• <strong>Designed role-based access controls</strong> across Redshift and Snowflake environments, <span class='highlight'>aligning with ISO 27001 policies</span> and improving audit traceability across data lineage paths<br/>• <strong>Led weekly client syncs</strong> to define data SLAs, set transformation logic, and <span class='highlight'>finalize 20+ KPIs for executive dashboards</span> used by healthcare, fintech, and logistics clients",
      "image": "images/adons-softech-logo.png", 
      "links": {
        "paper": null,
        "github": null,
        "status": null
      }
    },
    {
      "id": "cybage-software",
      "title": "Data Analyst",
      "authors": "Cybage Software | India", 
      "venue": "Jan 2020 - Dec 2022",
      "description": "<div class='impact-metrics'><span class='metric'>15+ E-commerce Platforms</span><span class='metric'>12% Conversion Rate Increase</span><span class='metric'>8 Hours Weekly Savings</span><span class='metric'>40% Error Reduction</span></div><br/><strong>Key Achievements:</strong><br/>• <strong>Analyzed clickstream data</strong> from over 15 e-commerce platforms using SQL and Python, <span class='highlight'>identifying user drop-off patterns and supporting marketing decisions that led to a 12% increase in average conversion rates</span><br/>• <strong>Developed interactive Tableau dashboards</strong> that visualized cross-regional customer behavior metrics for marketing teams, <span class='highlight'>directly impacting segmentation strategies across 6 APAC and North American business units</span><br/>• <strong>Automated Excel-based workflows</strong> using Python and Power BI, <span class='highlight'>cutting weekly manual reporting time by 8 hours</span> and improving report consistency across 3 internal departments<br/>• <strong>Built DAX-based data models</strong> in Power BI to track user churn and retention, <span class='highlight'>enabling product teams to test and validate feature changes with real-time data insights</span><br/>• <strong>Deployed data validation scripts</strong> to catch upstream anomalies in customer orders, <span class='highlight'>reducing transactional errors over a 3-month monitoring period</span><br/>• <strong>Defined and implemented data quality rules</strong> and anomaly checks using SQL for customer and inventory datasets, <span class='highlight'>resulting in a 40% decrease in reporting discrepancies</span>",
      "image": "images/cybage-software-logo-new.png",
      "links": {
        "paper": null, 
        "github": null,
        "status": null
      }
    }
  ],
  
  "skills": {
    "programming": [
      "Python",
      "Java", 
      "Scala",
      "SQL",
      "JavaScript",
      "Bash/Shell Scripting",
      "DAX"
    ],
    "bigDataTechnologies": [
      "Apache Spark",
      "PySpark",
      "Apache Kafka",
      "Hadoop",
      "Hive",
      "Flume",
      "HBase",
      "Apache Flink"
    ],
    "cloudPlatforms": [
      "AWS (S3, Redshift, Lambda, EC2, Glue)",
      "Azure (Data Lake Storage, Synapse, Functions)",
      "GCP (BigQuery)",
      "Snowflake",
      "Amazon Redshift"
    ],
    "databases": [
      "MySQL",
      "PostgreSQL", 
      "MongoDB",
      "Cassandra",
      "Oracle DB",
      "Relational Databases"
    ],
    "dataWarehousing": [
      "Snowflake",
      "Amazon Redshift",
      "Star Schema",
      "Snowflake Schema",
      "Dimensional Modeling"
    ],
    "dataProcessing": [
      "Apache Spark",
      "Apache Flink",
      "PySpark"
    ],
    "etlTools": [
      "Apache Airflow",
      "Informatica",
      "Talend",
      "dbt"
    ],
    "dataGovernance": [
      "Data Validation",
      "Data Cleansing",
      "Databricks",
      "Data Catalogs",
      "IAM (AWS/GCP)",
      "GDPR/CCPA Compliance"
    ],
    "versionControl": [
      "Git",
      "GitHub",
      "Bitbucket",
      "Jira",
      "Confluence"
    ],
    "containerization": [
      "Docker",
      "Kubernetes (EKS/GKE)",
      "Terraform",
      "GitLab CI/CD",
      "Jenkins",
      "Travis CI"
    ],
    "monitoring": [
      "Prometheus",
      "Grafana",
      "Splunk",
      "ELK Stack",
      "Elasticsearch"
    ],
    "methodologies": [
      "Agile",
      "Scrum",
      "CI/CD",
      "Continuous Delivery",
      "Stakeholder Collaboration",
      "Process Improvement"
    ],
    "visualization": [
      "Tableau",
      "Power BI",
      "Tabular Models",
      "Excel (Pivot Tables, Macros, VLOOKUP)"
    ],
    "softSkills": [
      "Problem Solving",
      "Attention to Details",
      "Best Practices",
      "Troubleshooting",
      "Code Review",
      "Technical Documentation"
    ]
  },
  
  "projects": [
    {
      "id": "nyc-property-analysis-capstone",
      "title": "NYC Property Sale Price Analysis", 
      "year": "2025",
      "category": "academic",
      "shortDescription": "Advanced ML capstone project analyzing 600K+ NYC property records with 82% prediction accuracy across 5 boroughs.",
      "detailedDescription": "<div class='project-metrics'><span class='metric'>600K+ Records</span><span class='metric'>82% R² Accuracy</span><span class='metric'>5 NYC Boroughs</span><span class='metric'>Random Forest ML</span></div><br/>Comprehensive capstone project (ADTA 5940) analyzing NYC real estate market using advanced machine learning techniques. Processed 600,000+ property sales records, implementing sophisticated data cleaning pipelines to handle extreme outliers and missing values. Built Random Forest and Gradient Boosting models achieving 82% R² accuracy in price prediction. Conducted borough-specific analysis revealing Manhattan's unique market characteristics and developed feature engineering strategies including price-per-square-foot ratios and building age calculations. Research findings provide actionable insights for real estate investors, policymakers, and urban planners.",
      "media": {
        "type": "image",
        "src": "images/nyc-property-analysis.png",
        "poster": null
      },
      "techStack": [
        "Python",
        "Pandas", 
        "Scikit-learn",
        "Random Forest",
        "Gradient Boosting",
        "Statistical Analysis",
        "Data Visualization"
      ],
      "metrics": [
        {"value": "82%", "label": "R² Accuracy"},
        {"value": "600K+", "label": "Records Processed"},
        {"value": "5", "label": "Boroughs Analyzed"}
      ],
      "links": {
        "website": null,
        "github": null,
        "details": "documents/academic/NYC_Property_Analysis_Capstone.docx"
      },
      "documentPath": "documents/academic/NYC_Property_Analysis_Capstone.docx",
      "isExpandable": true
    },
    {
      "id": "hotel-booking-analytics",
      "title": "Hotel Booking Analytics & Customer Behavior",
      "year": "2023", 
      "category": "academic",
      "shortDescription": "Team analytics project using ANOVA and regression models to analyze booking patterns and customer behavior optimization.",
      "detailedDescription": "<div class='project-metrics'><span class='metric'>ANOVA Analysis</span><span class='metric'>Regression Models</span><span class='metric'>Team Project</span><span class='metric'>Customer Insights</span></div><br/>Collaborative team project (Group 15) analyzing hotel booking patterns to understand customer behavior and optimize pricing strategies. Implemented comprehensive EDA on booking data, conducting ANOVA tests to examine the effect of meal types on lead times and market segments on stay duration. Developed multiple regression models including linear regression for special requests prediction and logistic regression for cancellation likelihood (66% accuracy). Applied statistical techniques to identify seasonal trends, customer segmentation patterns, and operational insights for hotel management decision-making.",
      "media": {
        "type": "image",
        "src": "images/hotel-booking-analytics.png",
        "poster": null
      },
      "techStack": [
        "Python",
        "Pandas",
        "ANOVA",
        "Linear Regression", 
        "Logistic Regression",
        "Statistical Modeling",
        "EDA"
      ],
      "metrics": [
        {"value": "66%", "label": "Model Accuracy"},
        {"value": "5", "label": "Team Members"},
        {"value": "3", "label": "Statistical Models"}
      ],
      "links": {
        "website": null,
        "github": null,
        "details": "documents/academic/Hotel_Booking_Analytics_Final_Report.docx"
      },
      "documentPath": "documents/academic/Hotel_Booking_Analytics_Final_Report.docx", 
      "isExpandable": true
    },
    {
      "id": "student-performance-analysis",
      "title": "Academic Performance Analysis by Gender & Discipline",
      "year": "2024",
      "category": "academic", 
      "shortDescription": "Data analytics case study examining GPA correlations across academic disciplines with gender-based performance insights.",
      "detailedDescription": "<div class='project-metrics'><span class='metric'>3 Academic Disciplines</span><span class='metric'>Gender Analysis</span><span class='metric'>GPA Correlation</span><span class='metric'>Data Visualization</span></div><br/>Individual case study project (ADTA 5820) analyzing academic performance patterns across Arts & Letters, Business & Economics, and Math & Science disciplines. Conducted comparative analysis revealing consistent female academic advantages across all fields, with Math & Science showing highest performance levels (3.71 GPA for females vs 3.54 for males). Developed insights for university admissions and student success programs, identifying Business & Economics as requiring targeted male student support initiatives. Created compelling data visualizations demonstrating the relationship between high school and college GPA performance by gender and discipline.",
      "media": {
        "type": "image", 
        "src": "images/student-performance-analysis.png",
        "poster": null
      },
      "techStack": [
        "Statistical Analysis",
        "Data Visualization",
        "Performance Metrics",
        "Comparative Analysis",
        "Educational Data",
        "Gender Studies"
      ],
      "metrics": [
        {"value": "3", "label": "Disciplines Analyzed"},
        {"value": "2", "label": "Gender Categories"},
        {"value": "6", "label": "Performance Metrics"}
      ],
      "links": {
        "website": null,
        "github": null,
        "details": "documents/academic/Student_Performance_Analysis_ADTA5820.docx"
      },
      "documentPath": "documents/academic/Student_Performance_Analysis_ADTA5820.docx",
      "isExpandable": true
    },
    {
      "id": "real-time-trading-system",
      "title": "Real-Time Financial Analysis Trading System",
      "year": "2024",
      "category": "github",
      "shortDescription": "Enterprise-grade distributed system for processing financial data with sub-second latency and microservices architecture.",
      "detailedDescription": "<div class='project-metrics'><span class='metric'>Sub-second Latency</span><span class='metric'>Microservices</span><span class='metric'>Distributed Computing</span><span class='metric'>Financial Data</span></div><br/>Scalable platform designed for processing real-time financial market data and generating trading signals using distributed computing principles. Implemented microservices architecture with stream processing capabilities for handling high-frequency market data feeds. Built trading indicator computation engine with WebSocket-based real-time notifications for critical market events. Designed for enterprise deployment with focus on reliability, scalability, and low-latency requirements essential for financial trading applications.",
      "media": {
        "type": "image",
        "src": "images/real-time-trading-system.png", 
        "poster": null
      },
      "techStack": [
        "Distributed Computing",
        "Microservices",
        "Stream Processing", 
        "WebSockets",
        "Financial APIs",
        "Real-time Analytics",
        "Trading Systems"
      ],
      "metrics": [
        {"value": "<1s", "label": "Latency"},
        {"value": "24/7", "label": "Uptime Target"},
        {"value": "Real-time", "label": "Processing"}
      ],
      "links": {
        "website": null,
        "github": "https://github.com/deepak-rambarki/real-time-financial-analysis-trading-system",
        "details": "#"
      },
      "isExpandable": true
    },
    {
      "id": "financial-advisor-llm", 
      "title": "Financial Advisor LLM with RAG Architecture",
      "year": "2024",
      "category": "github",
      "shortDescription": "Advanced AI chatbot using 3-pipeline architecture with vector database, streaming, and LLMOps best practices.",
      "detailedDescription": "<div class='project-metrics'><span class='metric'>3-Pipeline Architecture</span><span class='metric'>RAG System</span><span class='metric'>Vector Database</span><span class='metric'>LLMOps</span></div><br/>Comprehensive financial advisory chatbot implementing modern LLM deployment patterns with Retrieval-Augmented Generation (RAG). Features three-pipeline architecture: feature pipeline for data processing, training pipeline for model optimization, and inference pipeline for real-time responses. Utilizes Qdrant vector database for semantic search, Bytewax for streaming data processing, and PyTorch Transformers for LLM inference. Implements LLMOps best practices with Pydantic for data validation and WebSocket connections for real-time financial advisory interactions.",
      "media": {
        "type": "image",
        "src": "images/financial-advisor-llm.png",
        "poster": null
      },
      "techStack": [
        "Transformers",
        "PyTorch", 
        "Qdrant Vector DB",
        "Bytewax",
        "Pydantic",
        "WebSocket",
        "RAG Architecture"
      ],
      "metrics": [
        {"value": "3", "label": "Pipeline Architecture"},
        {"value": "RAG", "label": "AI System Type"},
        {"value": "Real-time", "label": "Response Speed"}
      ],
      "links": {
        "website": null,
        "github": "https://github.com/deepak-rambarki/financial_advisor_llm",
        "details": "#"
      },
      "isExpandable": true
    },
    {
      "id": "fraud-detection-ml-system",
      "title": "Fraud Detection Using Machine Learning",
      "year": "2024", 
      "category": "github",
      "shortDescription": "AWS SageMaker-powered fraud detection system with supervised/unsupervised models and real-time inference API.",
      "detailedDescription": "<div class='project-metrics'><span class='metric'>$898/month Architecture</span><span class='metric'>AWS SageMaker</span><span class='metric'>Real-time Inference</span><span class='metric'>XGBoost & RandomCutForest</span></div><br/>Enterprise-grade machine learning system deployed on AWS for real-time fraud detection in financial transactions. Combines supervised learning (XGBoost) with unsupervised anomaly detection (RandomCutForest) to handle imbalanced datasets effectively. Implements complete MLOps pipeline with SageMaker for model training and deployment, Kinesis for streaming data ingestion, and Lambda functions for serverless processing. Features REST API deployment via API Gateway with comprehensive cost analysis and regulatory compliance considerations.",
      "media": {
        "type": "image",
        "src": "images/fraud-detection-ml.png",
        "poster": null
      },
      "techStack": [
        "AWS SageMaker",
        "XGBoost",
        "RandomCutForest", 
        "Kinesis",
        "Lambda",
        "API Gateway",
        "MLOps"
      ],
      "metrics": [
        {"value": "$898", "label": "Monthly Cost"},
        {"value": "2", "label": "ML Models"},
        {"value": "REST", "label": "API Type"}
      ],
      "links": {
        "website": null,
        "github": "https://github.com/deepak-rambarki/fraud-detection-using-machine-learning", 
        "details": "#"
      },
      "isExpandable": true
    },
    {
      "id": "local-data-lakehouse",
      "title": "Local Data Lakehouse Architecture",
      "year": "2024",
      "category": "github",
      "shortDescription": "Complete data lakehouse implementation using Docker with MinIO, Apache Iceberg, and Trino for modern data architecture.",
      "detailedDescription": "<div class='project-metrics'><span class='metric'>Docker Architecture</span><span class='metric'>Apache Iceberg</span><span class='metric'>S3-Compatible Storage</span><span class='metric'>ACID Transactions</span></div><br/>Modern data lakehouse architecture designed for local development and testing, implementing industry-standard open table formats. Features MinIO for S3-compatible object storage, Apache Iceberg for schema evolution and ACID transaction support, and Trino as the distributed SQL query engine. Includes Hive Metastore with MariaDB backend for metadata management. Provides cost-effective alternative to cloud data warehouses while maintaining enterprise-level capabilities for data engineering experimentation and development.",
      "media": {
        "type": "image", 
        "src": "images/local-data-lakehouse.png",
        "poster": null
      },
      "techStack": [
        "Docker",
        "MinIO", 
        "Apache Iceberg",
        "Trino",
        "Hive Metastore",
        "MariaDB",
        "Data Lake Architecture"
      ],
      "metrics": [
        {"value": "5", "label": "Core Services"},
        {"value": "ACID", "label": "Transaction Support"},
        {"value": "Local", "label": "Deployment"}
      ],
      "links": {
        "website": null,
        "github": "https://github.com/deepak-rambarki/local-data-lakehouse",
        "details": "#"
      },
      "isExpandable": true
    },
    {
      "id": "dashboard-mock-sales",
      "title": "Mock Sales Analytics Dashboard",
      "year": "2024",
      "category": "github", 
      "shortDescription": "Professional BI dashboard with Python Dash featuring 4 visualizations, 6 KPIs, and full ETL pipeline with normalized database.",
      "detailedDescription": "<div class='project-metrics'><span class='metric'>4 Visualizations</span><span class='metric'>6 KPI Indicators</span><span class='metric'>ETL Pipeline</span><span class='metric'>Data Warehouse</span></div><br/>Business Intelligence dashboard showcasing data engineering and visualization capabilities using Python Dash and Plotly. Implements complete data pipeline from extraction through normalization to data warehouse storage with SQLite backend. Features interactive dropdowns for dynamic filtering, geographic performance monitoring, and profitability analysis across multiple business dimensions. Demonstrates full-stack data engineering skills including database design, ETL processing, and modern web-based dashboard development for business stakeholders.",
      "media": {
        "type": "image",
        "src": "images/dashboard-mock-sales.png",
        "poster": null
      },
      "techStack": [
        "Python",
        "Dash",
        "Plotly", 
        "SQLite",
        "Pandas",
        "ETL Pipelines",
        "Business Intelligence"
      ],
      "metrics": [
        {"value": "4", "label": "Visualizations"},
        {"value": "6", "label": "KPI Indicators"}, 
        {"value": "100%", "label": "Interactive"}
      ],
      "links": {
        "website": null,
        "github": "https://github.com/deepak-rambarki/dashboard_dash_mock_sales",
        "details": "#"
      },
      "isExpandable": true
    },
    {
      "id": "stock-analysis-dashboard",
      "title": "Real-Time Stock Market Analysis Dashboard",
      "year": "2024",
      "category": "github",
      "shortDescription": "Full-stack Power BI application with Financial Modeling Prep API integration for real-time market data and correlation analysis.",
      "detailedDescription": "<div class='project-metrics'><span class='metric'>Real-time API Integration</span><span class='metric'>Power BI</span><span class='metric'>Candlestick Charts</span><span class='metric'>Correlation Analysis</span></div><br/>Comprehensive stock market analysis platform integrating Financial Modeling Prep API with Power BI for real-time market insights. Features automated data refresh mechanisms, candlestick chart visualizations, and advanced correlation analysis for investment decision support. Implements DAX calculations for financial metrics and historical performance tracking. Demonstrates end-to-end data pipeline development from API consumption to interactive financial dashboard creation, providing actionable insights for investment portfolio management.",
      "media": {
        "type": "image",
        "src": "images/stock-analysis-dashboard.png",
        "poster": null
      },
      "techStack": [
        "Power BI",
        "Financial Modeling Prep API",
        "DAX",
        "Python",
        "Financial Data", 
        "Real-time Processing",
        "Investment Analytics"
      ],
      "metrics": [
        {"value": "Real-time", "label": "Data Updates"},
        {"value": "API", "label": "Data Source"},
        {"value": "Financial", "label": "Domain Focus"}
      ],
      "links": {
        "website": null,
        "github": "https://github.com/deepak-rambarki/stock-analysis",
        "details": "#"
      },
      "isExpandable": true
    }
  ],
  
  "certifications": [
  ],
  
  "education": [
    {
      "degree": "Master in Advanced Data Analytics",
      "institution": "University of North Texas",
      "location": "Denton, TX", 
      "year": "Aug 2023 – May 2025",
      "logo": "images/university-north-texas-logo.png"
    },
    {
      "degree": "Bachelor in Mechanical Engineering Honors",
      "institution": "Lovely Professional University",
      "location": "Punjab, India",
      "year": "Aug 2017 – Aug 2021",
      "logo": "images/lovely-professional-university-logo.png"
    }
  ],
  
  "siteConfig": {
    "siteTitle": "Deepak Rambarki - Senior Data Engineer",
    "favicon": null,
    "themeColors": {
      "primaryRed": "#2E86AB",
      "lightRed": "#3498DB", 
      "darkRed": "#1B4F72",
      "textDark": "#2C3E50",
      "textLight": "#6c757d",
      "bgLight": "#f8f9fa",
      "white": "#ffffff"
    },
    "domain": "deepak-rambarki.com",
    "googleAnalytics": null
  }
}